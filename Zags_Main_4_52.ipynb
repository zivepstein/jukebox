{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Zags Main 4.52",
      "provenance": [],
      "collapsed_sections": [
        "ZAGS4pnjbmI1"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAGS4j8u0gdN"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAGS4aH_BPGN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtK7KvgqCI-k"
      },
      "source": [
        "# Stopped mid level = 2? use continue\r\n",
        "# Stopped mid level = 1 or 0? use upsample\r\n",
        "\r\n",
        "lemode = 'primed'     # 'ancestral','primed','continue','cutcontinue','upsample'\r\n",
        "lemodel = '5b_lyrics'                          #5b_lyrics or '5b' or '1b_lyrics'\r\n",
        "\r\n",
        "lecount = 3\r\n",
        "lesample_length_in_seconds = 202\r\n",
        "lesampling_temperature = .98616616\r\n",
        "lehop = [.5,.5,.125]                 #default [.5,.5,.125], optimized [1,1,0.0625]\r\n",
        "\r\n",
        "lepath = '/content/gdrive/MyDrive/songsample'\r\n",
        "\r\n",
        "leprompt_length_in_seconds=10  \r\n",
        "leaudio_file = '/content/gdrive/MyDrive/song.wav'                    \r\n",
        "\r\n",
        "lecut = 70               # used only on cutcontinue\r\n",
        "transpose = [0,1,2]      # used only on cutcontinue [0,1,2] = default, ex [1,1,1] all samples are copied from item 1\r\n",
        "\r\n",
        "leexportlyrics = True\r\n",
        "leprogress = True\r\n",
        "leautorename = True\r\n",
        "\r\n",
        "leartist = \"blondie\"\r\n",
        "legenre = \"pop rock\"\r\n",
        "lelyrics = \"\"\"George Twisleton was the son \r\n",
        "of John Twisleton of Aula Barrow, Yorkshire. \r\n",
        "He was a lieutenant colonel \r\n",
        " in the parliamentary army \r\n",
        "and distinguished himself at \r\n",
        " the siege of Denbigh Castle. \r\n",
        " He was one of General Mytton's commissioners\r\n",
        " for receiving \r\n",
        " the surrender of the castle on 14 October 1646. \r\n",
        " He was made governor of Denbigh and ruled \r\n",
        " with a firm hand until the Restoration.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "lechunk_size = 16 \r\n",
        "lemax_batch_size = lecount\r\n",
        "lelower_batch_size = lechunk_size\r\n",
        "lelower_level_chunk_size = lechunk_size * 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-8cvPn3CO4s"
      },
      "source": [
        "# 1 Sample\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAGS4k1WCC_C"
      },
      "source": [
        "if lemode=='ancestral':\n",
        "  leprompt_length_in_seconds=None  \n",
        "  leaudio_file = None\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "\n",
        "codes_file=None\n",
        "\n",
        "!pip install git+https://github.com/openai/jukebox.git\n",
        "\n",
        "##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$#### autosave start\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "filex = \"/usr/local/lib/python3.6/dist-packages/jukebox/sample.py\"\n",
        "fin = open(filex, \"rt\")\n",
        "data = fin.read()\n",
        "fin.close()\n",
        "\n",
        "newtext = '''import fire\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "from termcolor import colored\n",
        "from datetime import datetime\n",
        "\n",
        "newtosample = True'''\n",
        "data = data.replace('import fire',newtext)\n",
        "\n",
        "newtext = '''starts = get_starts(total_length, prior.n_ctx, hop_length)\n",
        "        counterr = 0\n",
        "        x = None\n",
        "        for start in starts:'''\n",
        "data = data.replace('for start in get_starts(total_length, prior.n_ctx, hop_length):',newtext)\n",
        "\n",
        "newtext = '''global newtosample\n",
        "    newtosample = (new_tokens > 0)\n",
        "    if new_tokens <= 0:'''\n",
        "data = data.replace('if new_tokens <= 0:',newtext)\n",
        "\n",
        "newtext = '''counterr += 1\n",
        "            datea = datetime.now()\t\t\n",
        "            zs = sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)\t\t\t\n",
        "            if newtosample and counterr < len(starts):\n",
        "                del x; x = None; prior.cpu(); empty_cache()\n",
        "                x = prior.decode(zs[level:], start_level=level, bs_chunks=zs[level].shape[0])\n",
        "                logdir = f\"{hps.name}/level_{level}\"\n",
        "                if not os.path.exists(logdir):\n",
        "                    os.makedirs(logdir)\n",
        "                t.save(dict(zs=zs, labels=labels, sampling_kwargs=sampling_kwargs, x=x), f\"{logdir}/data.pth.tar\")\n",
        "                save_wav(logdir, x, hps.sr)\n",
        "                del x; prior.cuda(); empty_cache(); x = None\n",
        "            dateb = datetime.now()\n",
        "            timex = ((dateb-datea).total_seconds()/60.0)*(len(starts)-counterr)\n",
        "            print(f\"Step \" + colored(counterr,'blue') + \"/\" + colored( len(starts),'red') + \" ~ New to Sample: \" + str(newtosample) + \" ~ estimated remaining minutes: \" + (colored('???','yellow'), colored(timex,'magenta'))[counterr > 1 and newtosample])'''\n",
        "data = data.replace('zs = sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)',newtext)\n",
        "\n",
        "\n",
        "newtext = \"\"\"lepath=hps.name\n",
        "        if level==2:\n",
        "          for filex in glob(os.path.join(lepath + '/level_2','item_*.wav')):\n",
        "            os.rename(filex,filex.replace('item_',lepath.split('/')[-1] + '-'))\n",
        "        if level==1:\n",
        "          for filex in glob(os.path.join(lepath + '/level_1','item_*.wav')):\n",
        "            os.rename(filex,filex.replace('item_',lepath.split('/')[-1] + '-L1-'))\n",
        "        if level==0:\n",
        "          for filex in glob(os.path.join(lepath + '/level_0','item_*.wav')):\n",
        "            os.rename(filex,filex.replace('item_',lepath.split('/')[-1] + '-L0-'))\n",
        "        save_html(\"\"\"\n",
        "if leautorename:\n",
        "  data = data.replace('save_html(',newtext)\n",
        "\n",
        "if leexportlyrics == False:\n",
        "  data = data.replace('if alignments is None','#if alignments is None')\n",
        "  data = data.replace('alignments = get_alignment','#alignments = get_alignment')\n",
        "  data = data.replace('save_html(','#save_html(')\n",
        "\n",
        "if leprogress == False:\n",
        "  data = data.replace('print(f\"Step \" +','#print(f\"Step \" +')\n",
        "  \n",
        "fin = open(filex, \"wt\")\n",
        "fin.write(data)\n",
        "fin.close()\n",
        "##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$#### autosave end\n",
        "\n",
        "import jukebox\n",
        "import torch as t\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from IPython.display import Audio\n",
        "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
        "from jukebox.hparams import Hyperparams, setup_hparams\n",
        "from jukebox.sample import sample_single_window, _sample, \\\n",
        "                           sample_partial_window, upsample, \\\n",
        "                           load_prompts\n",
        "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "from jukebox.utils.torch_utils import empty_cache\n",
        "rank, local_rank, device = setup_dist_from_mpi()\n",
        "\n",
        "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
        "\n",
        "model = lemodel\n",
        "hps = Hyperparams()\n",
        "hps.sr = 44100\n",
        "hps.n_samples = lecount \n",
        "hps.name = lepath\n",
        "\n",
        "chunk_size = lechunk_size\n",
        "max_batch_size = lemax_batch_size\n",
        "\n",
        "hps.levels = 3\n",
        "hps.hop_fraction = lehop\n",
        "\n",
        "vqvae, *priors = MODELS[model]\n",
        "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
        "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)\n",
        "\n",
        "# Prime song creation using an arbitrary audio sample.\n",
        "mode = lemode\n",
        "codes_file=None\n",
        "audio_file = leaudio_file\n",
        "prompt_length_in_seconds=leprompt_length_in_seconds\n",
        "\n",
        "\n",
        "if os.path.exists(hps.name):\n",
        "  # Identify the lowest level generated and continue from there.\n",
        "  for level in [0, 1, 2]:\n",
        "    data = f\"{hps.name}/level_{level}/data.pth.tar\"\n",
        "    if os.path.isfile(data):\n",
        "      mode = mode if 'continue' in mode else 'upsample'\n",
        "      codes_file = data\n",
        "      print(mode + 'ing from level ' + str(level))\n",
        "      break\n",
        "print('mode is now '+mode)\n",
        "\n",
        "sample_hps = Hyperparams(dict(mode=mode, codes_file=codes_file, audio_file=audio_file, prompt_length_in_seconds=prompt_length_in_seconds))\n",
        "\n",
        "sample_length_in_seconds = lesample_length_in_seconds \n",
        "hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "assert hps.sample_length >= top_prior.n_ctx*top_prior.raw_to_tokens, f'Please choose a larger sampling rate'\n",
        "\n",
        "metas = [dict(artist = leartist,\n",
        "            genre = legenre,\n",
        "            total_length = hps.sample_length,\n",
        "            offset = 0,\n",
        "            lyrics = lelyrics,\n",
        "            ),\n",
        "          ] * hps.n_samples\n",
        "labels = [None, None, top_prior.labeller.get_batch_labels(metas, 'cuda')]\n",
        "\n",
        "\n",
        "#----------------------------------------------------------2\n",
        "\n",
        "\n",
        "sampling_temperature = lesampling_temperature\n",
        "lower_batch_size = lelower_batch_size\n",
        "max_batch_size = lemax_batch_size\n",
        "lower_level_chunk_size = lelower_level_chunk_size\n",
        "chunk_size = lechunk_size \n",
        "sampling_kwargs = [dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n",
        "                        chunk_size=lower_level_chunk_size),\n",
        "                    dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n",
        "                         chunk_size=lower_level_chunk_size),\n",
        "                    dict(temp=sampling_temperature, fp16=True, \n",
        "                         max_batch_size=max_batch_size, chunk_size=chunk_size)]\n",
        "\n",
        "if sample_hps.mode == 'ancestral':\n",
        "  zs = [t.zeros(hps.n_samples,0,dtype=t.long, device='cuda') for _ in range(len(priors))]\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "elif sample_hps.mode == 'upsample':\n",
        "  assert sample_hps.codes_file is not None\n",
        "  # Load codes.\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cuda() for z in data['zs']]\n",
        "  assert zs[-1].shape[0] == hps.n_samples, f\"Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}\"\n",
        "  del data\n",
        "  print('Falling through to the upsample step later in the notebook.')\n",
        "elif sample_hps.mode == 'primed':\n",
        "  assert sample_hps.audio_file is not None\n",
        "  audio_files = sample_hps.audio_file.split(',')\n",
        "  duration = (int(sample_hps.prompt_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "  x = load_prompts(audio_files, duration, hps)\n",
        "  zs = top_prior.encode(x, start_level=0, end_level=len(priors), bs_chunks=x.shape[0])\n",
        "  print(sample_hps.prompt_length_in_seconds)\n",
        "  print(hps.sr)\n",
        "  print(top_prior.raw_to_tokens)\n",
        "  print('aaaaaaaaaaaaaaaaaaaaaaaaaaaa 4.52')\n",
        "  print(duration)\n",
        "  print(audio_files)\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "elif sample_hps.mode == 'continue':\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cuda() for z in data['zs']]\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "elif sample_hps.mode == 'cutcontinue':\n",
        "  print('-------CUT INIT--------')\n",
        "  lecutlen = (int(lecut*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "  print(lecutlen)\n",
        "  data = t.load(codes_file, map_location='cpu')\n",
        "  zabaca = [z.cuda() for z in data['zs']]\n",
        "  print(zabaca)\n",
        "  assert zabaca[-1].shape[0] == hps.n_samples, f\"Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}\"\n",
        "  priorsz = [top_prior] * 3\n",
        "  top_raw_to_tokens = priorsz[-1].raw_to_tokens\n",
        "  assert lecutlen % top_raw_to_tokens == 0, f\"Cut-off duration {lecutlen} not an exact multiple of top_raw_to_tokens\"\n",
        "  assert lecutlen//top_raw_to_tokens <= zabaca[-1].shape[1], f\"Cut-off tokens {lecutlen//priorsz[-1].raw_to_tokens} longer than tokens {zs[-1].shape[1]} in saved codes\"\n",
        "  zabaca = [z[:,:lecutlen//prior.raw_to_tokens] for z, prior in zip(zabaca, priorsz)]\n",
        "  hps.sample_length = lecutlen\n",
        "  print(zabaca)\n",
        "  zs = _sample(zabaca, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "  del data\n",
        "  print('-------CUT OK--------')\n",
        "  hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zibica = [z.cuda() for z in data['zs']]\n",
        "  zubu = zibica[:]\n",
        "  if transpose != [0,1,2]:\n",
        "    zubu[2][0] = zibica[:][2][transpose[0]];zubu[2][1] = zibica[:][2][transpose[1]];zubu[2][2] = zibica[:][2][transpose[2]]\n",
        "    zubu[1][0] = zibica[:][1][transpose[0]];zubu[1][1] = zibica[:][1][transpose[1]];zubu[1][2] = zibica[:][1][transpose[2]]\n",
        "    zubu[0][0] = zibica[:][0][transpose[0]];zubu[0][1] = zibica[:][0][transpose[1]];zubu[0][2] = zibica[:][0][transpose[2]]\n",
        "  zubu = _sample(zubu, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "  print('-------CONTINUE AFTER CUT OK--------')\n",
        "  zs = zubu\n",
        "else:\n",
        "  raise ValueError(f'Unknown sample mode {sample_hps.mode}.')\n",
        "\n",
        "\n",
        "\n",
        "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAGS4pnjbmI1"
      },
      "source": [
        "# 2 Upsample\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAGS4LolpZ6w"
      },
      "source": [
        "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
        "del top_prior\n",
        "empty_cache()\n",
        "top_prior=None\n",
        "\n",
        "upsamplers = [make_prior(setup_hparams(prior, dict()), vqvae, 'cpu') for prior in priors[:-1]]\n",
        "labels[:2] = [prior.labeller.get_batch_labels(metas, 'cuda') for prior in upsamplers]\n",
        "\n",
        "zs = upsample(zs, labels, sampling_kwargs, [*upsamplers, top_prior], hps)\n",
        "print(datetime.now().strftime(\"%H:%M:%S\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}